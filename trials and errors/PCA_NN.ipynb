{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMA: A Dataset For Music Analysis\n",
    "\n",
    "MichaÃ«l Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n",
    "\n",
    "## Baselines\n",
    "\n",
    "* This notebook evalutates standard classifiers from scikit-learn on the provided features.\n",
    "* Moreover, it evaluates Deep Learning models on both audio and spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/dtgo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/dotenv/main.py:24: UserWarning: Not loading  - it doesn't exist.\n",
      "  warnings.warn(\"Not loading %s - it doesn't exist.\" % dotenv_path)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13129, 249)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((106574, 52), (106574, 518), (13129, 249))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "\n",
    "tracks = utils.load('data/tracks.csv')\n",
    "features = utils.load('data/features.csv')\n",
    "echonest = utils.load('data/echonest.csv')\n",
    "print(echonest.shape)\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()\n",
    "\n",
    "tracks.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough Echonest features: (13129, 767)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 52), (25000, 518))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = tracks.index[tracks['set', 'subset'] <= 'medium']\n",
    "\n",
    "assert subset.isin(tracks.index).all()\n",
    "assert subset.isin(features.index).all()\n",
    "\n",
    "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "features_all = features.loc[subset]\n",
    "\n",
    "tracks.shape, features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19922 training examples, 2505 validation examples, 2573 testing examples\n",
      "Top genres (35): [' ', '-', '/', 'B', 'C', 'E', 'F', 'H', 'I', 'J', 'L', 'O', 'P', 'R', 'S', 'T', 'a', 'c', 'd', 'e', 'g', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x', 'y', 'z']\n",
      "All genres (151): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 30, 31, 32, 33, 36, 37, 38, 41, 42, 43, 45, 46, 47, 49, 53, 58, 63, 64, 65, 66, 70, 71, 74, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 97, 98, 100, 101, 102, 103, 107, 109, 111, 113, 117, 118, 125, 130, 137, 138, 166, 167, 169, 171, 172, 174, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 214, 224, 232, 236, 240, 247, 250, 267, 286, 296, 297, 311, 314, 322, 337, 359, 360, 361, 362, 374, 378, 400, 401, 404, 428, 439, 440, 441, 442, 443, 456, 468, 491, 495, 502, 504, 514, 524, 538, 539, 542, 580, 602, 619, 651, 659, 695, 741, 763, 808, 810, 811, 906, 1032, 1060, 1193, 1235]\n"
     ]
    }
   ],
   "source": [
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n",
    "\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genre_top']).classes_)\n",
    "#genres = list(tracks['track', 'genre_top'].unique())\n",
    "print('Top genres ({}): {}'.format(len(genres), genres))\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n",
    "print('All genres ({}): {}'.format(len(genres), genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Multiple classifiers and feature sets\n",
    "\n",
    "Todo:\n",
    "* Cross-validation for hyper-parameters.\n",
    "* Dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(tracks, features, columns, multi_label=False, verbose=False):\n",
    "    if not multi_label:\n",
    "        # Assign an integer value to each genre.\n",
    "        enc = LabelEncoder()\n",
    "        labels = tracks['track', 'genre_top']\n",
    "    else:\n",
    "        # Create an indicator matrix.\n",
    "        enc = MultiLabelBinarizer()\n",
    "        labels = tracks['track', 'genres_all']\n",
    "        #labels = tracks['track', 'genres']\n",
    "\n",
    "    # Split in training, validation and testing sets.\n",
    "    y_train = enc.fit_transform(labels[train])\n",
    "    y_val = enc.transform(labels[val])\n",
    "    y_test = enc.transform(labels[test])\n",
    "    X_train = features.loc[train, columns].as_matrix()\n",
    "    X_val = features.loc[val, columns].as_matrix()\n",
    "    X_test = features.loc[test, columns].as_matrix()\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    \n",
    "    # Standardize features by removing the mean and scaling to unit variance.\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_val)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    return y_train, y_val, y_test, X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106574, 518)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse',\n",
    "             'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast',\n",
    "             'spectral_rolloff', 'tonnetz', 'zcr']\n",
    "    \n",
    "features.loc[train, columns].shape\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Single genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### softmax regression ########\n",
    "## loss function of softmax regression\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] #number of training example\n",
    "    y_mat = oneHotIt(y)  #convert the interger class coding in to a one-hot representation\n",
    "    scores = np.dot(x,w) #comput raw class scores given input and current weight\n",
    "    prob = softmax(scores) #perform softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(x.T, (y_mat - prob)) + lam*w #And compute the gradient for the loss\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "## unidimentional array of labels into a one-hot varient\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m),(Y,np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "## determine the probabilities and predictions for each class when given a set of input data:\n",
    "def getProbsAndPreds(someX,w):\n",
    "    probs = softmax(np.dot(someX,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs, preds\n",
    "\n",
    "##\n",
    "def getAccuracy(someX,someY,w):\n",
    "    prob, prede = getProbsAndPreds(someX,w)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifiers_features(classifiers, feature_sets, multi_label=False):\n",
    "    columns = list(classifiers.keys()).insert(0, 'dim')\n",
    "    scores = pd.DataFrame(columns=columns, index=feature_sets.keys())\n",
    "    times = pd.DataFrame(columns=classifiers.keys(), index=feature_sets.keys())\n",
    "\n",
    "#     X_train_all = np.empty([19922,1])\n",
    "\n",
    "#     X_test_all = np.empty([2573,1])\n",
    "#     for fset_name, fset in tqdm_notebook(feature_sets.items(), desc='features'):\n",
    "#         y_train, y_val, y_test, X_train, X_val, X_test = pre_process(tracks, features_all, fset, multi_label)\n",
    "#         #scores.loc[fset_name, 'dim'] = X_train.shape[1]\n",
    "#         X_train_all = np.hstack((X_train_all, X_train))\n",
    "#         X_test_all = np.hstack((X_test_all, X_test))     \n",
    "#         ##clf_name = 'kNN'\n",
    "#         ##clf = classifiers[clf_name]\n",
    "#         ##for clf_name, clf in classifiers.items():  # tqdm_notebook(classifiers.items(), desc='classifiers', leave=False):\n",
    "#         ##t = time.process_time()\n",
    "#         ##clf.fit(X_train, y_train)\n",
    "#         ##score = clf.score(X_test, y_test)\n",
    "#         ##scores.loc[fset_name, clf_name] = score\n",
    "#         ##times.loc[fset_name, clf_name] = time.process_time() - t  \n",
    "#     X_train_all = np.delete(X_train_all,[0],axis=1)\n",
    "#     X_test_all = np.delete(X_test_all,[0],axis=1)\n",
    "#     print(X_train_all.shape, X_test_all.shape)\n",
    "#     #return scores, times\n",
    "    \n",
    "    columns = ['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse',\n",
    "             'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast',\n",
    "             'spectral_rolloff', 'tonnetz', 'zcr']\n",
    "    \n",
    "    y_train, y_val, y_test, X_train, X_val, X_test = pre_process(tracks, features, columns, multi_label=False, verbose=False);\n",
    "    \n",
    "\n",
    "    #print(\"x size, y size: \", X_train_all.shape, y_train.shape)\n",
    "    model = SelectKBest(k=275)\n",
    "    fit = model.fit(X_train, y_train)\n",
    "    X_train = fit.transform(X_train)\n",
    "    X_test = fit.transform(X_test)\n",
    "    \n",
    "    M = X_train.shape[0]/10\n",
    "    X_learning = np.empty(10)\n",
    "    Y_train_curve = np.empty(10)\n",
    "    Y_test_curve = np.empty(10)\n",
    "    for j in range(10):\n",
    "        MJ = int(M*j)\n",
    "        X_train_this = np.delete(X_train,np.s_[0:MJ],axis=0)\n",
    "        Y_train_this = np.delete(y_train,np.s_[0:MJ],axis=0)\n",
    "        \n",
    "\n",
    "        \n",
    "        w = np.zeros([X_train_this.shape[1], 16])   #len(np.unique(Y_train_this)=16\n",
    "        lam = 1\n",
    "        iterations = 5000\n",
    "        learningRate = 1e-4\n",
    "        losses = []\n",
    "        \n",
    "        \n",
    "        for i in range(0,iterations):\n",
    "            loss, grad = getLoss(w,X_train_this,Y_train_this,lam)\n",
    "            losses.append(loss)\n",
    "            w = w - (learningRate * grad)\n",
    "\n",
    "            \n",
    "        #print(loss)\n",
    "        \n",
    "        X_learning[j] = (10-j)*M\n",
    "        Y_train_curve[j] = getAccuracy(X_train_this,Y_train_this,w)\n",
    "        Y_test_curve[j] = getAccuracy(X_test,y_test,w)\n",
    "        print('Training Accuracy:', Y_train_curve[j])\n",
    "        print('Test Accuracy:', Y_test_curve[j])\n",
    "      \n",
    "    plt.plot(X_learning, Y_train_curve, linewidth = 2.0, color = 'red')\n",
    "    plt.plot(X_learning, Y_test_curve, linewidth = 2.0, color = 'blue')\n",
    "    plt.show()\n",
    "    #print('Training Accuracy:', getAccuracy(X_train,y_train,w))\n",
    "    #print('Test Accuracy:', getAccuracy(X_test,y_test,w))\n",
    "\n",
    "\n",
    "def format_scores(scores):\n",
    "    def highlight(s):\n",
    "        is_max = s == max(s[1:])\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "    scores = scores.style.apply(highlight, axis=1)\n",
    "    return scores.format('{:.2%}', subset=pd.IndexSlice[:, scores.columns[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape:  (19922, 518) (19922,)\n",
    "2.72712255201\n",
    "Training Accuracy: 0.479670715792\n",
    "Test Accuracy: 0.47260007773\n",
    "shape:  (17930, 518) (17930,)\n",
    "2.72705942451\n",
    "Training Accuracy: 0.482208588957\n",
    "Test Accuracy: 0.471822774971\n",
    "shape:  (15938, 518) (15938,)\n",
    "2.72656854697\n",
    "Training Accuracy: 0.482933868741\n",
    "Test Accuracy: 0.472211426351\n",
    "shape:  (13946, 518) (13946,)\n",
    "2.72629859806\n",
    "Training Accuracy: 0.485730675462\n",
    "Test Accuracy: 0.474154683249\n",
    "shape:  (11954, 518) (11954,)\n",
    "2.72631561022\n",
    "Training Accuracy: 0.487870168981\n",
    "Test Accuracy: 0.474931986009\n",
    "shape:  (9961, 518) (9961,)\n",
    "2.72590218012\n",
    "Training Accuracy: 0.492219656661\n",
    "Test Accuracy: 0.475709288768\n",
    "shape:  (7969, 518) (7969,)\n",
    "2.72544865504\n",
    "Training Accuracy: 0.492784540093\n",
    "Test Accuracy: 0.471822774971\n",
    "shape:  (5977, 518) (5977,)\n",
    "2.72425531267\n",
    "Training Accuracy: 0.497574033796\n",
    "Test Accuracy: 0.474543334629\n",
    "shape:  (3985, 518) (3985,)\n",
    "2.72301800337\n",
    "Training Accuracy: 0.502885821832\n",
    "Test Accuracy: 0.474931986009\n",
    "shape:  (1993, 518) (1993,)\n",
    "2.72654753076\n",
    "Training Accuracy: 0.508278976417\n",
    "Test Accuracy: 0.482316362223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8b76e875fb43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m'all_non-echonest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m })\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtest_classifiers_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#ipd.display(format_scores(scores))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-886616d7a2a8>\u001b[0m in \u001b[0;36mtest_classifiers_features\u001b[0;34m(classifiers, feature_sets, multi_label)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0fcd12bb4863>\u001b[0m in \u001b[0;36mgetLoss\u001b[0;34m(w, x, y, lam)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHotIt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#convert the interger class coding in to a one-hot representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#comput raw class scores given input and current weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#perform softmax on these scores to get their probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We then find the loss of the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m \u001b[0;31m#And compute the gradient for the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0fcd12bb4863>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'LR': LogisticRegression(),\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "}\n",
    "for name in features.columns.levels[0]:\n",
    "    feature_sets[name] = name\n",
    "feature_sets.update({\n",
    "    'mfcc/contrast': ['mfcc', 'spectral_contrast'],\n",
    "    'mfcc/contrast/chroma': ['mfcc', 'spectral_contrast', 'chroma_cens'],\n",
    "    'mfcc/contrast/centroid': ['mfcc', 'spectral_contrast', 'spectral_centroid'],\n",
    "    'mfcc/contrast/chroma/centroid': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid'],\n",
    "    'mfcc/contrast/chroma/centroid/tonnetz': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'tonnetz'],\n",
    "    'mfcc/contrast/chroma/centroid/zcr': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'zcr'],\n",
    "    'all_non-echonest': list(features.columns.levels[0])\n",
    "})\n",
    "test_classifiers_features(classifiers, feature_sets)\n",
    "\n",
    "#ipd.display(format_scores(scores))\n",
    "#ipd.display(times.style.format('{:.4f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax regression for all features\n",
    "2.72712255201\n",
    "Training Accuracy: 0.479670715792\n",
    "Test Accuracy: 0.47260007773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy: 0.528711976709\n",
    "Test Accuracy: 0.510299261562\n",
    "Training Accuracy: 0.531622978249\n",
    "Test Accuracy: 0.5134084726\n",
    "Training Accuracy: 0.534320491906\n",
    "Test Accuracy: 0.511465215702\n",
    "Training Accuracy: 0.534848702137\n",
    "Test Accuracy: 0.510299261562\n",
    "Training Accuracy: 0.537309687134\n",
    "Test Accuracy: 0.511465215702\n",
    "Training Accuracy: 0.539303282803\n",
    "Test Accuracy: 0.512242518461\n",
    "Training Accuracy: 0.541849667461\n",
    "Test Accuracy: 0.512242518461\n",
    "Training Accuracy: 0.543751045675\n",
    "Test Accuracy: 0.5134084726\n",
    "Training Accuracy: 0.554579673777\n",
    "Test Accuracy: 0.511465215702\n",
    "Training Accuracy: 0.567486201706\n",
    "Test Accuracy: 0.517294986397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse',\n",
       "       'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast',\n",
       "       'spectral_rolloff', 'tonnetz', 'zcr'],\n",
       "      dtype='object', name='feature')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets\n",
    "features.columns.levels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çæ²¹ç½ç»å¼å§å¦ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19922, 518)\n"
     ]
    }
   ],
   "source": [
    "columns = ['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse',\n",
    "             'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast',\n",
    "             'spectral_rolloff', 'tonnetz', 'zcr']\n",
    "    \n",
    "y_train, y_val, y_test, X_train, X_val, X_test = pre_process(tracks, features, columns, multi_label=False, verbose=False);\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19922, 518)\n",
      "(19922,)\n",
      "(2573, 518)\n",
      "(2573,)\n",
      "(19922, 300)\n",
      "(19922,)\n",
      "(2505, 300)\n",
      "(2505,)\n"
     ]
    }
   ],
   "source": [
    "### PCA\n",
    "#X_train = PCA(n_components=518).fit_transform(X_train)\n",
    "#X_test = PCA(n_components=518).fit_transform(X_test)\n",
    "### feature selection\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "model = SelectKBest(k=300)\n",
    "fit = model.fit(X_train, y_train)\n",
    "X_train = fit.transform(X_train)\n",
    "X_test = fit.transform(X_val)\n",
    "y_test = y_val\n",
    "\n",
    "#X_train = SelectKBest(k=200).fit_transform(X_train, y_train)\n",
    "#X_test = SelectKBest(k=200).fit_transform(X_test, y_test)\n",
    "#pipe = make_pipeline(\n",
    "#    SelectKBest(k=200)\n",
    "#)\n",
    "#pipe.fit(X_train, y_train)\n",
    "#X_test = pipe.predict(X_test)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19922/19922 [==============================] - 1s 30us/step - loss: 1.8160 - acc: 0.4673\n",
      "Epoch 2/20\n",
      "19922/19922 [==============================] - 0s 21us/step - loss: 1.4566 - acc: 0.5445\n",
      "Epoch 3/20\n",
      "19922/19922 [==============================] - 0s 21us/step - loss: 1.3281 - acc: 0.6117\n",
      "Epoch 4/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.2515 - acc: 0.6319\n",
      "Epoch 5/20\n",
      "19922/19922 [==============================] - 0s 19us/step - loss: 1.1991 - acc: 0.6449\n",
      "Epoch 6/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.1606 - acc: 0.6530\n",
      "Epoch 7/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.1305 - acc: 0.6596\n",
      "Epoch 8/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.1055 - acc: 0.6642\n",
      "Epoch 9/20\n",
      "19922/19922 [==============================] - 0s 17us/step - loss: 1.0852 - acc: 0.6704\n",
      "Epoch 10/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0667 - acc: 0.6735\n",
      "Epoch 11/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0506 - acc: 0.6787\n",
      "Epoch 12/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0351 - acc: 0.6820\n",
      "Epoch 13/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0203 - acc: 0.6864\n",
      "Epoch 14/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0080 - acc: 0.6896\n",
      "Epoch 15/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 0.9945 - acc: 0.6939\n",
      "Epoch 16/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 0.9823 - acc: 0.6964\n",
      "Epoch 17/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 0.9701 - acc: 0.7001\n",
      "Epoch 18/20\n",
      "19922/19922 [==============================] - 0s 17us/step - loss: 0.9593 - acc: 0.7043\n",
      "Epoch 19/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 0.9474 - acc: 0.7070\n",
      "Epoch 20/20\n",
      "19922/19922 [==============================] - 0s 17us/step - loss: 0.9359 - acc: 0.7116\n",
      "2505/2505 [==============================] - 0s 28us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1062243672426113, 0.66027944111776449]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(100,input_dim=300),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(33,input_dim=100),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(16),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(y_train, num_classes=16)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(X_train, one_hot_labels, epochs=20, batch_size=200)\n",
    "ylabels = keras.utils.to_categorical(y_test, num_classes=16)\n",
    "model.evaluate(X_test, ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential([\n",
    "    Dense(320,input_dim=518),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(32,input_dim=320),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(16),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "0.8050\n",
    "0.6129\n",
    "\n",
    "67.03%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ°ççæ²¹ç½ç»ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "     'all': ['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse',\n",
    "           'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast',\n",
    "           'spectral_rolloff', 'tonnetz', 'zcr']\n",
    "    \n",
    "y_train, y_val, y_test, X_train, X_val, X_test = pre_process(tracks, features_all, fset, multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=250 must be between 0 and n_features=2 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-d7fda918588f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    408\u001b[0m             raise ValueError(\"n_components=%r must be between 0 and \"\n\u001b[1;32m    409\u001b[0m                              \u001b[0;34m\"n_features=%r with svd_solver='full'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                              % (n_components, n_features))\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Center data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=250 must be between 0 and n_features=2 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "X_train = PCA(n_components=250).fit_transform(X_train)\n",
    "X_test = PCA(n_components=250).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19922, 250)\n",
      "(19922,)\n",
      "(2573, 250)\n",
      "(2573,)\n",
      "Epoch 1/20\n",
      "19922/19922 [==============================] - 1s 33us/step - loss: 2.0057 - acc: 0.3901\n",
      "Epoch 2/20\n",
      "19922/19922 [==============================] - 0s 15us/step - loss: 1.5376 - acc: 0.5310\n",
      "Epoch 3/20\n",
      "19922/19922 [==============================] - 0s 17us/step - loss: 1.3604 - acc: 0.5878\n",
      "Epoch 4/20\n",
      "19922/19922 [==============================] - 0s 14us/step - loss: 1.2622 - acc: 0.6333\n",
      "Epoch 5/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.1974 - acc: 0.6486\n",
      "Epoch 6/20\n",
      "19922/19922 [==============================] - 0s 19us/step - loss: 1.1525 - acc: 0.6562\n",
      "Epoch 7/20\n",
      "19922/19922 [==============================] - 0s 18us/step - loss: 1.1194 - acc: 0.6650\n",
      "Epoch 8/20\n",
      "19922/19922 [==============================] - 0s 18us/step - loss: 1.0932 - acc: 0.6700\n",
      "Epoch 9/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0704 - acc: 0.6751\n",
      "Epoch 10/20\n",
      "19922/19922 [==============================] - 0s 14us/step - loss: 1.0513 - acc: 0.6786\n",
      "Epoch 11/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 1.0335 - acc: 0.6827\n",
      "Epoch 12/20\n",
      "19922/19922 [==============================] - 0s 14us/step - loss: 1.0184 - acc: 0.6874\n",
      "Epoch 13/20\n",
      "19922/19922 [==============================] - 0s 15us/step - loss: 1.0038 - acc: 0.6917\n",
      "Epoch 14/20\n",
      "19922/19922 [==============================] - 0s 14us/step - loss: 0.9895 - acc: 0.6952\n",
      "Epoch 15/20\n",
      "19922/19922 [==============================] - 0s 19us/step - loss: 0.9763 - acc: 0.6988\n",
      "Epoch 16/20\n",
      "19922/19922 [==============================] - 0s 18us/step - loss: 0.9630 - acc: 0.7027\n",
      "Epoch 17/20\n",
      "19922/19922 [==============================] - 0s 15us/step - loss: 0.9513 - acc: 0.7065\n",
      "Epoch 18/20\n",
      "19922/19922 [==============================] - 0s 15us/step - loss: 0.9388 - acc: 0.7108\n",
      "Epoch 19/20\n",
      "19922/19922 [==============================] - 0s 16us/step - loss: 0.9277 - acc: 0.7131\n",
      "Epoch 20/20\n",
      "19922/19922 [==============================] - 0s 17us/step - loss: 0.9157 - acc: 0.7178\n",
      "2573/2573 [==============================] - 0s 52us/step\n",
      "Epoch 1/20\n",
      "17930/17930 [==============================] - 1s 37us/step - loss: 1.9776 - acc: 0.4195\n",
      "Epoch 2/20\n",
      "17930/17930 [==============================] - 0s 14us/step - loss: 1.5378 - acc: 0.5453\n",
      "Epoch 3/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 1.3475 - acc: 0.6081\n",
      "Epoch 4/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 1.2557 - acc: 0.6313\n",
      "Epoch 5/20\n",
      "17930/17930 [==============================] - 0s 15us/step - loss: 1.1975 - acc: 0.6466\n",
      "Epoch 6/20\n",
      "17930/17930 [==============================] - 0s 19us/step - loss: 1.1554 - acc: 0.6578\n",
      "Epoch 7/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 1.1230 - acc: 0.6637\n",
      "Epoch 8/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 1.0968 - acc: 0.6712\n",
      "Epoch 9/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 1.0746 - acc: 0.6748\n",
      "Epoch 10/20\n",
      "17930/17930 [==============================] - 0s 15us/step - loss: 1.0552 - acc: 0.6799\n",
      "Epoch 11/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 1.0384 - acc: 0.6842\n",
      "Epoch 12/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 1.0219 - acc: 0.6871\n",
      "Epoch 13/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 1.0077 - acc: 0.6908\n",
      "Epoch 14/20\n",
      "17930/17930 [==============================] - 0s 13us/step - loss: 0.9939 - acc: 0.6940\n",
      "Epoch 15/20\n",
      "17930/17930 [==============================] - 0s 15us/step - loss: 0.9810 - acc: 0.6967\n",
      "Epoch 16/20\n",
      "17930/17930 [==============================] - 0s 16us/step - loss: 0.9681 - acc: 0.7016\n",
      "Epoch 17/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 0.9561 - acc: 0.7056\n",
      "Epoch 18/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 0.9446 - acc: 0.7090\n",
      "Epoch 19/20\n",
      "17930/17930 [==============================] - 0s 15us/step - loss: 0.9329 - acc: 0.7136\n",
      "Epoch 20/20\n",
      "17930/17930 [==============================] - 0s 17us/step - loss: 0.9221 - acc: 0.7167\n",
      "2573/2573 [==============================] - 0s 53us/step\n",
      "Epoch 1/20\n",
      "15938/15938 [==============================] - 1s 40us/step - loss: 2.0207 - acc: 0.3954\n",
      "Epoch 2/20\n",
      "15938/15938 [==============================] - 0s 15us/step - loss: 1.5943 - acc: 0.5087\n",
      "Epoch 3/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 1.4027 - acc: 0.5760\n",
      "Epoch 4/20\n",
      "15938/15938 [==============================] - 0s 13us/step - loss: 1.2971 - acc: 0.6159\n",
      "Epoch 5/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 1.2271 - acc: 0.6378\n",
      "Epoch 6/20\n",
      "15938/15938 [==============================] - 0s 13us/step - loss: 1.1773 - acc: 0.6529\n",
      "Epoch 7/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 1.1397 - acc: 0.6607\n",
      "Epoch 8/20\n",
      "15938/15938 [==============================] - 0s 13us/step - loss: 1.1098 - acc: 0.6653\n",
      "Epoch 9/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 1.0854 - acc: 0.6734\n",
      "Epoch 10/20\n",
      "15938/15938 [==============================] - 0s 17us/step - loss: 1.0646 - acc: 0.6789\n",
      "Epoch 11/20\n",
      "15938/15938 [==============================] - 0s 13us/step - loss: 1.0461 - acc: 0.6813\n",
      "Epoch 12/20\n",
      "15938/15938 [==============================] - 0s 15us/step - loss: 1.0298 - acc: 0.6869\n",
      "Epoch 13/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 1.0146 - acc: 0.6899\n",
      "Epoch 14/20\n",
      "15938/15938 [==============================] - 0s 19us/step - loss: 1.0010 - acc: 0.6924\n",
      "Epoch 15/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 0.9876 - acc: 0.6974\n",
      "Epoch 16/20\n",
      "15938/15938 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7004\n",
      "Epoch 17/20\n",
      "15938/15938 [==============================] - 0s 19us/step - loss: 0.9623 - acc: 0.7033\n",
      "Epoch 18/20\n",
      "15938/15938 [==============================] - 0s 18us/step - loss: 0.9503 - acc: 0.7066\n",
      "Epoch 19/20\n",
      "15938/15938 [==============================] - 0s 19us/step - loss: 0.9388 - acc: 0.7112\n",
      "Epoch 20/20\n",
      "15938/15938 [==============================] - 0s 16us/step - loss: 0.9266 - acc: 0.7141\n",
      "2573/2573 [==============================] - 0s 58us/step\n",
      "Epoch 1/20\n",
      "13946/13946 [==============================] - 1s 50us/step - loss: 2.2350 - acc: 0.3004\n",
      "Epoch 2/20\n",
      "13946/13946 [==============================] - 0s 16us/step - loss: 1.6643 - acc: 0.4926\n",
      "Epoch 3/20\n",
      "13946/13946 [==============================] - 0s 15us/step - loss: 1.4598 - acc: 0.5451\n",
      "Epoch 4/20\n",
      "13946/13946 [==============================] - 0s 14us/step - loss: 1.3490 - acc: 0.6058\n",
      "Epoch 5/20\n",
      "13946/13946 [==============================] - 0s 16us/step - loss: 1.2693 - acc: 0.6345\n",
      "Epoch 6/20\n",
      "13946/13946 [==============================] - 0s 14us/step - loss: 1.2106 - acc: 0.6481\n",
      "Epoch 7/20\n",
      "13946/13946 [==============================] - 0s 13us/step - loss: 1.1661 - acc: 0.6583\n",
      "Epoch 8/20\n",
      "13946/13946 [==============================] - 0s 13us/step - loss: 1.1314 - acc: 0.6623\n",
      "Epoch 9/20\n",
      "13946/13946 [==============================] - 0s 13us/step - loss: 1.1028 - acc: 0.6699\n",
      "Epoch 10/20\n",
      "13946/13946 [==============================] - 0s 14us/step - loss: 1.0789 - acc: 0.6736\n",
      "Epoch 11/20\n",
      "13946/13946 [==============================] - 0s 20us/step - loss: 1.0576 - acc: 0.6790\n",
      "Epoch 12/20\n",
      "13946/13946 [==============================] - 0s 19us/step - loss: 1.0401 - acc: 0.6817\n",
      "Epoch 13/20\n",
      "13946/13946 [==============================] - 0s 16us/step - loss: 1.0229 - acc: 0.6855\n",
      "Epoch 14/20\n",
      "13946/13946 [==============================] - 0s 18us/step - loss: 1.0079 - acc: 0.6902\n",
      "Epoch 15/20\n",
      "13946/13946 [==============================] - 0s 17us/step - loss: 0.9935 - acc: 0.6959\n",
      "Epoch 16/20\n",
      "13946/13946 [==============================] - 0s 18us/step - loss: 0.9797 - acc: 0.6977\n",
      "Epoch 17/20\n",
      "13946/13946 [==============================] - 0s 16us/step - loss: 0.9666 - acc: 0.7033\n",
      "Epoch 18/20\n",
      "13946/13946 [==============================] - 0s 13us/step - loss: 0.9541 - acc: 0.7063\n",
      "Epoch 19/20\n",
      "13946/13946 [==============================] - 0s 14us/step - loss: 0.9418 - acc: 0.7105\n",
      "Epoch 20/20\n",
      "13946/13946 [==============================] - 0s 18us/step - loss: 0.9295 - acc: 0.7145\n",
      "2573/2573 [==============================] - 0s 85us/step\n",
      "Epoch 1/20\n",
      "11954/11954 [==============================] - 1s 51us/step - loss: 2.1055 - acc: 0.4034\n",
      "Epoch 2/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.6720 - acc: 0.4895\n",
      "Epoch 3/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.4813 - acc: 0.5353\n",
      "Epoch 4/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.3733 - acc: 0.5831\n",
      "Epoch 5/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.2994 - acc: 0.6123\n",
      "Epoch 6/20\n",
      "11954/11954 [==============================] - 0s 21us/step - loss: 1.2432 - acc: 0.6323\n",
      "Epoch 7/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.1977 - acc: 0.6503\n",
      "Epoch 8/20\n",
      "11954/11954 [==============================] - 0s 15us/step - loss: 1.1605 - acc: 0.6602\n",
      "Epoch 9/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.1292 - acc: 0.6671\n",
      "Epoch 10/20\n",
      "11954/11954 [==============================] - 0s 19us/step - loss: 1.1032 - acc: 0.6733\n",
      "Epoch 11/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.0806 - acc: 0.6798\n",
      "Epoch 12/20\n",
      "11954/11954 [==============================] - 0s 17us/step - loss: 1.0602 - acc: 0.6843\n",
      "Epoch 13/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.0417 - acc: 0.6886\n",
      "Epoch 14/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 1.0260 - acc: 0.6917\n",
      "Epoch 15/20\n",
      "11954/11954 [==============================] - 0s 17us/step - loss: 1.0104 - acc: 0.6971\n",
      "Epoch 16/20\n",
      "11954/11954 [==============================] - 0s 17us/step - loss: 0.9956 - acc: 0.7003\n",
      "Epoch 17/20\n",
      "11954/11954 [==============================] - 0s 18us/step - loss: 0.9826 - acc: 0.7026\n",
      "Epoch 18/20\n",
      "11954/11954 [==============================] - 0s 17us/step - loss: 0.9693 - acc: 0.7080\n",
      "Epoch 19/20\n",
      "11954/11954 [==============================] - 0s 17us/step - loss: 0.9567 - acc: 0.7134\n",
      "Epoch 20/20\n",
      "11954/11954 [==============================] - 0s 19us/step - loss: 0.9441 - acc: 0.7156\n",
      "2573/2573 [==============================] - 0s 86us/step\n",
      "Epoch 1/20\n",
      "9961/9961 [==============================] - 1s 63us/step - loss: 2.1882 - acc: 0.3502\n",
      "Epoch 2/20\n",
      "9961/9961 [==============================] - 0s 17us/step - loss: 1.7792 - acc: 0.4723\n",
      "Epoch 3/20\n",
      "9961/9961 [==============================] - 0s 19us/step - loss: 1.5722 - acc: 0.5101\n",
      "Epoch 4/20\n",
      "9961/9961 [==============================] - 0s 18us/step - loss: 1.4371 - acc: 0.5773\n",
      "Epoch 5/20\n",
      "9961/9961 [==============================] - 0s 20us/step - loss: 1.3434 - acc: 0.6186\n",
      "Epoch 6/20\n",
      "9961/9961 [==============================] - 0s 18us/step - loss: 1.2774 - acc: 0.6371\n",
      "Epoch 7/20\n",
      "9961/9961 [==============================] - 0s 15us/step - loss: 1.2271 - acc: 0.6452\n",
      "Epoch 8/20\n",
      "9961/9961 [==============================] - 0s 15us/step - loss: 1.1863 - acc: 0.6566\n",
      "Epoch 9/20\n",
      "9961/9961 [==============================] - 0s 16us/step - loss: 1.1528 - acc: 0.6668\n",
      "Epoch 10/20\n",
      "9961/9961 [==============================] - 0s 15us/step - loss: 1.1244 - acc: 0.6734\n",
      "Epoch 11/20\n",
      "9961/9961 [==============================] - 0s 17us/step - loss: 1.1005 - acc: 0.6791\n",
      "Epoch 12/20\n",
      "9961/9961 [==============================] - 0s 17us/step - loss: 1.0781 - acc: 0.6862\n",
      "Epoch 13/20\n",
      "9961/9961 [==============================] - 0s 14us/step - loss: 1.0582 - acc: 0.6897\n",
      "Epoch 14/20\n",
      "9961/9961 [==============================] - 0s 14us/step - loss: 1.0402 - acc: 0.6932\n",
      "Epoch 15/20\n",
      "9961/9961 [==============================] - 0s 14us/step - loss: 1.0234 - acc: 0.6988\n",
      "Epoch 16/20\n",
      "9961/9961 [==============================] - 0s 20us/step - loss: 1.0083 - acc: 0.7044\n",
      "Epoch 17/20\n",
      "9961/9961 [==============================] - 0s 16us/step - loss: 0.9942 - acc: 0.7047\n",
      "Epoch 18/20\n",
      "9961/9961 [==============================] - 0s 20us/step - loss: 0.9795 - acc: 0.7109\n",
      "Epoch 19/20\n",
      "9961/9961 [==============================] - 0s 14us/step - loss: 0.9665 - acc: 0.7119\n",
      "Epoch 20/20\n",
      "9961/9961 [==============================] - 0s 14us/step - loss: 0.9540 - acc: 0.7167\n",
      "2573/2573 [==============================] - 0s 66us/step\n",
      "Epoch 1/20\n",
      "7969/7969 [==============================] - 1s 83us/step - loss: 2.1444 - acc: 0.3734\n",
      "Epoch 2/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.7897 - acc: 0.4775\n",
      "Epoch 3/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.6080 - acc: 0.4967\n",
      "Epoch 4/20\n",
      "7969/7969 [==============================] - 0s 17us/step - loss: 1.4876 - acc: 0.5313\n",
      "Epoch 5/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.4013 - acc: 0.5673\n",
      "Epoch 6/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.3334 - acc: 0.6070\n",
      "Epoch 7/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.2814 - acc: 0.6330\n",
      "Epoch 8/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.2371 - acc: 0.6511\n",
      "Epoch 9/20\n",
      "7969/7969 [==============================] - 0s 17us/step - loss: 1.1992 - acc: 0.6582\n",
      "Epoch 10/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.1670 - acc: 0.6673\n",
      "Epoch 11/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.1384 - acc: 0.6742\n",
      "Epoch 12/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.1128 - acc: 0.6795\n",
      "Epoch 13/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.0892 - acc: 0.6842\n",
      "Epoch 14/20\n",
      "7969/7969 [==============================] - 0s 19us/step - loss: 1.0701 - acc: 0.6903\n",
      "Epoch 15/20\n",
      "7969/7969 [==============================] - 0s 17us/step - loss: 1.0508 - acc: 0.6934\n",
      "Epoch 16/20\n",
      "7969/7969 [==============================] - 0s 13us/step - loss: 1.0334 - acc: 0.6987\n",
      "Epoch 17/20\n",
      "7969/7969 [==============================] - 0s 15us/step - loss: 1.0177 - acc: 0.7042\n",
      "Epoch 18/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 1.0026 - acc: 0.7057\n",
      "Epoch 19/20\n",
      "7969/7969 [==============================] - 0s 18us/step - loss: 0.9881 - acc: 0.7111\n",
      "Epoch 20/20\n",
      "7969/7969 [==============================] - 0s 14us/step - loss: 0.9739 - acc: 0.7109\n",
      "2573/2573 [==============================] - 0s 68us/step\n",
      "Epoch 1/20\n",
      "5977/5977 [==============================] - 1s 98us/step - loss: 2.3892 - acc: 0.2839\n",
      "Epoch 2/20\n",
      "5977/5977 [==============================] - 0s 18us/step - loss: 1.9702 - acc: 0.4723\n",
      "Epoch 3/20\n",
      "5977/5977 [==============================] - 0s 15us/step - loss: 1.7712 - acc: 0.4812\n",
      "Epoch 4/20\n",
      "5977/5977 [==============================] - 0s 22us/step - loss: 1.6264 - acc: 0.4895\n",
      "Epoch 5/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.5220 - acc: 0.5218\n",
      "Epoch 6/20\n",
      "5977/5977 [==============================] - 0s 18us/step - loss: 1.4428 - acc: 0.5499\n",
      "Epoch 7/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.3807 - acc: 0.5754\n",
      "Epoch 8/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.3289 - acc: 0.5965\n",
      "Epoch 9/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.2850 - acc: 0.6155\n",
      "Epoch 10/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.2469 - acc: 0.6358\n",
      "Epoch 11/20\n",
      "5977/5977 [==============================] - 0s 18us/step - loss: 1.2133 - acc: 0.6510\n",
      "Epoch 12/20\n",
      "5977/5977 [==============================] - 0s 17us/step - loss: 1.1829 - acc: 0.6605\n",
      "Epoch 13/20\n",
      "5977/5977 [==============================] - 0s 17us/step - loss: 1.1566 - acc: 0.6712\n",
      "Epoch 14/20\n",
      "5977/5977 [==============================] - 0s 17us/step - loss: 1.1313 - acc: 0.6793\n",
      "Epoch 15/20\n",
      "5977/5977 [==============================] - 0s 17us/step - loss: 1.1088 - acc: 0.6830\n",
      "Epoch 16/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.0888 - acc: 0.6876\n",
      "Epoch 17/20\n",
      "5977/5977 [==============================] - 0s 15us/step - loss: 1.0701 - acc: 0.6910\n",
      "Epoch 18/20\n",
      "5977/5977 [==============================] - 0s 19us/step - loss: 1.0527 - acc: 0.6970\n",
      "Epoch 19/20\n",
      "5977/5977 [==============================] - 0s 18us/step - loss: 1.0362 - acc: 0.6998\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5977/5977 [==============================] - 0s 20us/step - loss: 1.0198 - acc: 0.7045\n",
      "2573/2573 [==============================] - 0s 100us/step\n",
      "Epoch 1/20\n",
      "3985/3985 [==============================] - 1s 146us/step - loss: 2.7639 - acc: 0.0996\n",
      "Epoch 2/20\n",
      "3985/3985 [==============================] - 0s 13us/step - loss: 2.2739 - acc: 0.4118\n",
      "Epoch 3/20\n",
      "3985/3985 [==============================] - 0s 14us/step - loss: 2.0283 - acc: 0.4718\n",
      "Epoch 4/20\n",
      "3985/3985 [==============================] - 0s 15us/step - loss: 1.8589 - acc: 0.4853\n",
      "Epoch 5/20\n",
      "3985/3985 [==============================] - 0s 16us/step - loss: 1.7287 - acc: 0.4959\n",
      "Epoch 6/20\n",
      "3985/3985 [==============================] - 0s 17us/step - loss: 1.6261 - acc: 0.5046\n",
      "Epoch 7/20\n",
      "3985/3985 [==============================] - ETA: 0s - loss: 1.5504 - acc: 0.517 - 0s 17us/step - loss: 1.5462 - acc: 0.5189\n",
      "Epoch 8/20\n",
      "3985/3985 [==============================] - 0s 16us/step - loss: 1.4812 - acc: 0.5405\n",
      "Epoch 9/20\n",
      "3985/3985 [==============================] - 0s 16us/step - loss: 1.4271 - acc: 0.5644\n",
      "Epoch 10/20\n",
      "3985/3985 [==============================] - 0s 14us/step - loss: 1.3798 - acc: 0.5857\n",
      "Epoch 11/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.3387 - acc: 0.6090\n",
      "Epoch 12/20\n",
      "3985/3985 [==============================] - 0s 20us/step - loss: 1.3028 - acc: 0.6248\n",
      "Epoch 13/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.2707 - acc: 0.6349\n",
      "Epoch 14/20\n",
      "3985/3985 [==============================] - 0s 19us/step - loss: 1.2420 - acc: 0.6427\n",
      "Epoch 15/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.2155 - acc: 0.6524\n",
      "Epoch 16/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.1909 - acc: 0.6587\n",
      "Epoch 17/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.1685 - acc: 0.6685\n",
      "Epoch 18/20\n",
      "3985/3985 [==============================] - 0s 18us/step - loss: 1.1471 - acc: 0.6753\n",
      "Epoch 19/20\n",
      "3985/3985 [==============================] - 0s 19us/step - loss: 1.1269 - acc: 0.6778\n",
      "Epoch 20/20\n",
      "3985/3985 [==============================] - 0s 17us/step - loss: 1.1077 - acc: 0.6821\n",
      "2573/2573 [==============================] - 0s 96us/step\n",
      "Epoch 1/20\n",
      "1993/1993 [==============================] - 1s 311us/step - loss: 2.6799 - acc: 0.0858\n",
      "Epoch 2/20\n",
      "1993/1993 [==============================] - 0s 20us/step - loss: 2.3037 - acc: 0.3136\n",
      "Epoch 3/20\n",
      "1993/1993 [==============================] - 0s 17us/step - loss: 2.1353 - acc: 0.4370\n",
      "Epoch 4/20\n",
      "1993/1993 [==============================] - 0s 18us/step - loss: 2.0282 - acc: 0.4571\n",
      "Epoch 5/20\n",
      "1993/1993 [==============================] - 0s 16us/step - loss: 1.9438 - acc: 0.4822\n",
      "Epoch 6/20\n",
      "1993/1993 [==============================] - 0s 16us/step - loss: 1.8691 - acc: 0.4927\n",
      "Epoch 7/20\n",
      "1993/1993 [==============================] - 0s 22us/step - loss: 1.7992 - acc: 0.4992\n",
      "Epoch 8/20\n",
      "1993/1993 [==============================] - 0s 20us/step - loss: 1.7343 - acc: 0.5013\n",
      "Epoch 9/20\n",
      "1993/1993 [==============================] - 0s 23us/step - loss: 1.6767 - acc: 0.5068\n",
      "Epoch 10/20\n",
      "1993/1993 [==============================] - 0s 21us/step - loss: 1.6238 - acc: 0.5153\n",
      "Epoch 11/20\n",
      "1993/1993 [==============================] - 0s 19us/step - loss: 1.5757 - acc: 0.5243\n",
      "Epoch 12/20\n",
      "1993/1993 [==============================] - 0s 21us/step - loss: 1.5314 - acc: 0.5409\n",
      "Epoch 13/20\n",
      "1993/1993 [==============================] - 0s 20us/step - loss: 1.4919 - acc: 0.5539\n",
      "Epoch 14/20\n",
      "1993/1993 [==============================] - 0s 19us/step - loss: 1.4556 - acc: 0.5670\n",
      "Epoch 15/20\n",
      "1993/1993 [==============================] - 0s 20us/step - loss: 1.4218 - acc: 0.5795\n",
      "Epoch 16/20\n",
      "1993/1993 [==============================] - 0s 19us/step - loss: 1.3917 - acc: 0.5961\n",
      "Epoch 17/20\n",
      "1993/1993 [==============================] - 0s 22us/step - loss: 1.3626 - acc: 0.6061\n",
      "Epoch 18/20\n",
      "1993/1993 [==============================] - 0s 19us/step - loss: 1.3353 - acc: 0.6232\n",
      "Epoch 19/20\n",
      "1993/1993 [==============================] - 0s 20us/step - loss: 1.3104 - acc: 0.6307\n",
      "Epoch 20/20\n",
      "1993/1993 [==============================] - 0s 19us/step - loss: 1.2867 - acc: 0.6412\n",
      "2573/2573 [==============================] - 0s 96us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a209590f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "M = X_train.shape[0]/10\n",
    "X_learning = np.empty(10)\n",
    "Y_train_curve = np.empty(10)\n",
    "Y_test_curve = np.empty(10)\n",
    "for j in range(10):\n",
    "    MJ = int(M*j)\n",
    "    \n",
    "    model = Sequential([\n",
    "    Dense(100,input_dim=250),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(32,input_dim=100),\n",
    "    Activation('sigmoid'),\n",
    "    Dense(16),\n",
    "    Activation('softmax'),\n",
    "\n",
    "    ])\n",
    "\n",
    "# For a multi-class classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_this = np.delete(X_train,np.s_[0:MJ],axis=0)\n",
    "    Y_train_this = np.delete(y_train,np.s_[0:MJ],axis=0)\n",
    "    X_learning[j] = (10-j)*M\n",
    "    \n",
    "    one_hot_labels = keras.utils.to_categorical(Y_train_this, num_classes=16)\n",
    "    trainingObj = model.fit(X_train_this, one_hot_labels, epochs=20, batch_size=200)\n",
    "    Y_train_curve[j] = trainingObj.history['acc'][19]   \n",
    "    ylabels = keras.utils.to_categorical(y_test, num_classes=16)\n",
    "    testingObj = model.evaluate(X_test, ylabels)\n",
    "    Y_test_curve[j] = testingObj[1]\n",
    "    \n",
    "\n",
    "plt.plot(X_learning, Y_train_curve, linewidth = 2.0, color = 'red')\n",
    "plt.plot(X_learning, Y_test_curve, linewidth = 2.0, color = 'blue')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Generate dummy data\n",
    "# data = X_train  # 19922,518\n",
    "# labels = y_train # 19922,1\n",
    "\n",
    "# # Convert labels to categorical one-hot encoding\n",
    "# one_hot_labels = keras.utils.to_categorical(labels, num_classes=16)\n",
    "\n",
    "# # Train the model, iterating on the data in batches of 32 samples\n",
    "# a = model.fit(data, one_hot_labels, epochs=20, batch_size=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79635579  0.79057446  0.7867361   0.78380897  0.77313033  0.77100692\n",
      "  0.75969381  0.7579053   0.7455458   0.72754642]\n",
      "[ 0.62883793  0.6195103   0.62767198  0.62456277  0.62223086  0.61251457\n",
      "  0.62223086  0.60746211  0.60979401  0.57403809]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_curve)\n",
    "print(Y_test_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836311614629\n"
     ]
    }
   ],
   "source": [
    "b = a.history['acc'][19]\n",
    "#b[19]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2573/2573 [==============================] - 0s 37us/step\n",
      "[1.46771096395603, 0.60124368442666243]\n"
     ]
    }
   ],
   "source": [
    "ylabels = keras.utils.to_categorical(y_test, num_classes=16)\n",
    "x = model.evaluate(X_test, ylabels)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60124368442666243"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multiple genres\n",
    "\n",
    "Todo:\n",
    "* Ignore rare genres? Count them higher up in the genre tree? On the other hand it's not much tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2efe3de4ddb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_classifiers_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mipd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1f65f738fbd3>\u001b[0m in \u001b[0;36mtest_classifiers_features\u001b[0;34m(classifiers, feature_sets, multi_label)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0fcd12bb4863>\u001b[0m in \u001b[0;36mgetLoss\u001b[0;34m(w, x, y, lam)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#number of training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHotIt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#convert the interger class coding in to a one-hot representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#comput raw class scores given input and current weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#perform softmax on these scores to get their probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#We then find the loss of the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    #LogisticRegression(),\n",
    "    'LR': OneVsRestClassifier(LogisticRegression()),\n",
    "    #'SVC': OneVsRestClassifier(SVC()),\n",
    "    #'MLP': MLPClassifier(max_iter=700),\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "#    'echonest_audio': ('echonest', 'audio_features'),\n",
    "#    'echonest_temporal': ('echonest', 'temporal_features'),\n",
    "    'mfcc': 'mfcc',\n",
    "    'mfcc/contrast/chroma/centroid/tonnetz': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'tonnetz'],\n",
    "    'mfcc/contrast/chroma/centroid/zcr': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'zcr'],\n",
    "}\n",
    "\n",
    "scores, times = test_classifiers_features(classifiers, feature_sets, multi_label=True)\n",
    "\n",
    "ipd.display(format_scores(scores))\n",
    "ipd.display(times.style.format('{:.4f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Deep learning on raw audio\n",
    "\n",
    "Other architectures:\n",
    "* [Learning Features of Music from Scratch (MusicNet)](https://arxiv.org/abs/1611.09827), John Thickstun, Zaid Harchaoui, Sham Kakade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_onehot = MultiLabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load audio samples in parallel using `multiprocessing` so as to maximize CPU usage when decoding MP3s and making some optional pre-processing. There are multiple ways to load a waveform from a compressed MP3:\n",
    "* librosa uses audioread in the backend which can use many native libraries, e.g. ffmpeg\n",
    "    * resampling is very slow --> use `kaiser_fast`\n",
    "    * does not work with multi-processing, for keras `fit_generator()`\n",
    "* pydub is a high-level interface for audio modification, uses ffmpeg to load\n",
    "    * store a temporary `.wav`\n",
    "* directly pipe ffmpeg output\n",
    "    * fastest method\n",
    "* [pyAV](https://github.com/mikeboers/PyAV) may be a fastest alternative by linking to ffmpeg libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ac664a324b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Just be sure that everything is fine. Multiprocessing is tricky to debug.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFfmpegLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_audio_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mSampleLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sample_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFfmpegLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSampleLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Stanford Document/courses/2017Fall/CS229/FinalProject/fma/utils.py\u001b[0m in \u001b[0;36mget_audio_path\u001b[0;34m(audio_dir, track_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_audio_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mtid_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:06d}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Just be sure that everything is fine. Multiprocessing is tricky to debug.\n",
    "utils.FfmpegLoader().load(utils.get_audio_path(AUDIO_DIR, 2))\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, labels_onehot, utils.FfmpegLoader())\n",
    "SampleLoader(train, batch_size=2).__next__()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'sched_getaffinity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7892d182cb7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Keras parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNB_WORKER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched_getaffinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of usables CPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pickle_safe'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nb_worker'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNB_WORKER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_q_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'sched_getaffinity'"
     ]
    }
   ],
   "source": [
    "# Keras parameters.\n",
    "NB_WORKER = len(os.sched_getaffinity(0))  # number of usables CPUs\n",
    "params = {'pickle_safe': True, 'nb_worker': NB_WORKER, 'max_q_size': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fully connected neural network\n",
    "\n",
    "* Two layers with 10 hiddens is no better than random, ~11%.\n",
    "\n",
    "Optimize data loading to be CPU / GPU bound, not IO bound. Larger batches means reduced training time, so increase batch time until memory exhaustion. Number of workers and queue size have no influence on speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: (59953,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_shape=(59953,), units=1000)`\n",
      "  \n",
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=100)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=35)`\n",
      "  if sys.path[0] == '':\n",
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/dtgo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.bui..., 19922, epochs=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dtgo/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/dtgo/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dtgo/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 630, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"/Users/dtgo/Stanford Document/courses/2017Fall/CS229/FinalProject/fma/utils.py\", line 335, in __next__\n",
      "    self.X[i] = self.loader.load(get_audio_path(audio_dir, tid))\n",
      "  File \"/Users/dtgo/Stanford Document/courses/2017Fall/CS229/FinalProject/fma/utils.py\", line 232, in get_audio_path\n",
      "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')\n",
      "  File \"/Users/dtgo/anaconda3/lib/python3.6/posixpath.py\", line 78, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-25409b0095fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSampleLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSampleLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSampleLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2081\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=2000)\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, labels_onehot, loader)\n",
    "print('Dimensionality: {}'.format(loader.shape))\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(output_dim=1000, input_shape=loader.shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=64), train.size, nb_epoch=2, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(val, batch_size=64), val.size, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=64), test.size, **params)\n",
    "#Y = model.predict_generator(SampleLoader(test, batch_size=64), test.size, **params);\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Convolutional neural network\n",
    "\n",
    "* Architecture: [End-to-end learning for music audio](http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202014/papers/p7014-dieleman.pdf), Sander Dieleman, Benjamin Schrauwen.\n",
    "* Missing: track segmentation and class averaging (majority voting)\n",
    "* Compared with log-scaled mel-spectrograms instead of strided convolution as first layer.\n",
    "* Larger net: http://benanne.github.io/2014/08/05/spotify-cnns.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=16000)\n",
    "#loader = utils.LibrosaLoader(sampling_rate=16000)\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, labels_onehot, loader)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Reshape((-1, 1), input_shape=loader.shape))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(128, 512, subsample_length=512))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "print(model.output_shape)\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = keras.optimizers.Adam()#lr=1e-5)#, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=10), train.size, nb_epoch=20, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(val, batch_size=10), val.size, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=10), test.size, **params)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Deep learning on extracted audio features\n",
    "\n",
    "Look at:\n",
    "* Pre-processing in Keras: https://github.com/keunwoochoi/kapre\n",
    "* Convolutional Recurrent Neural Networks for Music Classification: https://github.com/keunwoochoi/icassp_2017\n",
    "* Music Auto-Tagger: https://github.com/keunwoochoi/music-auto_tagging-keras\n",
    "* Pre-processor: https://github.com/bmcfee/pumpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ConvNet on MFCC\n",
    "\n",
    "* Architecture: [Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network](http://www.iaeng.org/publication/IMECS2010/IMECS2010_pp546-550.pdf), Tom LH. Li, Antoni B. Chan and Andy HW. Chun\n",
    "* Missing: track segmentation and majority voting.\n",
    "* Best seen: 17.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MfccLoader(utils.Loader):\n",
    "    raw_loader = utils.FfmpegLoader(sampling_rate=22050)\n",
    "    #shape = (13, 190)  # For segmented tracks.\n",
    "    shape = (13, 2582)\n",
    "    def load(self, filename):\n",
    "        import librosa\n",
    "        x = self.raw_loader.load(filename)\n",
    "        # Each MFCC frame spans 23ms on the audio signal with 50% overlap with the adjacent frames.\n",
    "        mfcc = librosa.feature.mfcc(x, sr=22050, n_mfcc=13, n_fft=512, hop_length=256)\n",
    "        return mfcc\n",
    "\n",
    "loader = MfccLoader()\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, labels_onehot, loader)\n",
    "loader.load(utils.get_audio_path(AUDIO_DIR, 2))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Reshape((*loader.shape, 1),  input_shape=loader.shape))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(3, 13, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(15, 1, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv2D(65, 1, 10, subsample=(1, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(labels_onehot.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(1e-3)#lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = keras.optimizers.Adam()#lr=1e-5)#\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(SampleLoader(train, batch_size=16), train.size, nb_epoch=20, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(val, batch_size=16), val.size, **params)\n",
    "loss = model.evaluate_generator(SampleLoader(test, batch_size=16), test.size, **params)\n",
    "#Y = model.predict_generator(loader, test.size, pickle_safe=True, nb_worker=NB_WORKER, max_q_size=5)\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
